{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np  # Required version 1.19.2\n",
    "import pandas as pd # Required version 1.1.3\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "class NBC:\n",
    "    \"\"\"\n",
    "    Naive Bayes Classifier\n",
    "    conditional: {\n",
    "        \"varname\": {\n",
    "            label_class\n",
    "            y0: ...\n",
    "            y1: {\n",
    "                feature condition probability on y1\n",
    "                    {\n",
    "                        name: p1\n",
    "                        name2: p2\n",
    "                    }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    prior: {\n",
    "        y1: p,\n",
    "        y2: p\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_params={}):\n",
    "        self.accuracy = 0\n",
    "        self.label_rectifier = lambda x: x\n",
    "        self.priors = {}\n",
    "        self.conditional = {}\n",
    "        self.categories = {}\n",
    "        self.label_name = \"\"\n",
    "        self.feature_names = []\n",
    "        self.model_params = model_params\n",
    "\n",
    "    def config(self, model_params):\n",
    "        # Config parameters for model\n",
    "        self.model_params = model_params\n",
    "\n",
    "    def train(self, train_dataset=None, label_name=\"\", train_data=None, train_label=None, max_discretize_count=10, sample_frac=0.5, partition=\"sqrt\", q=10, smoothing=True):\n",
    "        \"\"\"\n",
    "        All of the inputs are in pandas dataframe\n",
    "        train_dataset: training dataset combining data and label\n",
    "        train_data: training data in pandas dataframe\n",
    "        train_label: training label in pandas dataframe\n",
    "        max_discretize_count: if a feature contains feature counts over this value,\n",
    "            will classify data into ceil(sqrt(n)) bins where n is the number of entries\n",
    "        sample_frac: percent of data to be considered during counting unique feature value\n",
    "        partition: method to partition continuous data, can use either \"sqrt\" or \"qcut\"\n",
    "        q: bypass parameter into pd.qcut(), see it for for information\n",
    "        smoothing: laplace smoothing on the data to prevent zero probability\n",
    "        \"\"\"\n",
    "\n",
    "        # 0. Create dataset\n",
    "        try:\n",
    "            self.label_name = label_name\n",
    "            self.feature_names = train_dataset.columns.to_list()\n",
    "            self.feature_names.remove(label_name)\n",
    "        except AttributeError:\n",
    "            # No dataset provided, try to combined train_data and train_label\n",
    "            self.feature_names = train_data.columns\n",
    "            self.label_name = train_label.columns[0]\n",
    "            train_dataset = pd.concat([train_data, train_label], axis=1)\n",
    "\n",
    "        # 1. Initialize conditional dictionary\n",
    "        feature_names = self.feature_names\n",
    "        self.conditional = dict([(feature_name, {}) for feature_name in feature_names])\n",
    "\n",
    "        # 2. Initialize prior dictionary\n",
    "        self.label_name = label_name\n",
    "        self.priors = dict.fromkeys(train_dataset[self.label_name].unique())\n",
    "\n",
    "        # 3. Partition continuous variable\n",
    "        n = train_dataset.shape[0]\n",
    "        bins_count = int(np.ceil(n**0.5))\n",
    "        sample_data = train_dataset.sample(frac=sample_frac)\n",
    "        for feature_name in feature_names:\n",
    "            unique_count = sample_data[feature_name].unique().size\n",
    "            # Partition feature if it is larger than threshold\n",
    "            if unique_count > max_discretize_count or self.model_params[\"continuous\"].get(feature_name, False) == True:\n",
    "                if partition == \"sqrt\":\n",
    "                    train_dataset[feature_name] = pd.cut(train_dataset[feature_name], bins_count)\n",
    "                elif partition == \"qcut\":\n",
    "                    train_dataset[feature_name] = pd.qcut(train_dataset[feature_name], q=q, duplicates=\"drop\")\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid partition scheme for continuous variable\")\n",
    "\n",
    "                # Get categories to store in order to identify\n",
    "                # which category the new value in predict is\n",
    "                self.categories[feature_name] = train_dataset[feature_name].cat.categories\n",
    "\n",
    "        # 4. Calculate conditional and prior\n",
    "        # Prior\n",
    "        self.calculate_prior(train_dataset)\n",
    "\n",
    "        # conditional\n",
    "        self.calculate_conditional(train_dataset, smoothing=smoothing)\n",
    "\n",
    "    def calculate_conditional(self, train_dataset, smoothing=True):\n",
    "        label_values = self.priors.keys()\n",
    "        for feature in self.feature_names:\n",
    "            # Calculate the probability conditioning on label value for each feature value\n",
    "            # using value_counts()\n",
    "            feature_label_pair = train_dataset[[feature, label_name]]\n",
    "            feature_values = None\n",
    "            if smoothing and self.categories.get(feature, None) is None:\n",
    "                # Do not smooth interval feature as it will be handled by min and max\n",
    "                feature_values = self.model_params[\"smoothing_params\"][feature]\n",
    "            else:\n",
    "                feature_values = feature_label_pair[feature].unique()   # All possible value for this feature\n",
    "\n",
    "            feature_conditional = dict.fromkeys(label_values)\n",
    "            for label_value in label_values:\n",
    "                # Get feature-single_label\n",
    "                # pair such that (X, y=yi)\n",
    "                feature_single_label = feature_label_pair[feature_label_pair[label_name] == label_value]\n",
    "                feature_single_label = feature_single_label.reset_index(drop=True)\n",
    "\n",
    "                # Count count(y = yi)\n",
    "                N = feature_single_label.shape[0]\n",
    "\n",
    "                # Count feature unique value\n",
    "                k = len(feature_values)\n",
    "\n",
    "                # Use the following conditional possibility formula\n",
    "                # P(X = xi | y = yj) = count(xi and yj)/count(yj)\n",
    "                # If smoothing,\n",
    "                # P(X = xi | y = yj) = (count(xi and yj) + 1)/(count(yj) + count(feature_value))\n",
    "\n",
    "                # Count (X = xi and y = yj) and store in dict\n",
    "                feature_count = feature_single_label[feature].value_counts().to_dict()\n",
    "\n",
    "                # Calculate freq\n",
    "                for val in feature_values:\n",
    "                    if smoothing:\n",
    "                        feature_count[val] = (feature_count.get(val, 0) + 1) / (N + k)\n",
    "                    else:\n",
    "                        feature_count[val] = feature_count.get(val, 0) / N\n",
    "                feature_conditional[label_value] = feature_count\n",
    "            self.conditional[feature] = feature_conditional\n",
    "\n",
    "    def calculate_prior(self, train_dataset):\n",
    "        self.priors = train_dataset[self.label_name].value_counts(normalize=True).to_dict()\n",
    "\n",
    "    def evaluate(self, test_dataset, label_name):\n",
    "        test_data = test_dataset.drop(label_name, axis=1)\n",
    "        test_label = test_dataset[[label_name]]\n",
    "\n",
    "        size = test_data.shape[0]\n",
    "        loss_zero_one = 0\n",
    "        loss_squared = 0\n",
    "        test_label = self.label_rectifier(test_label)\n",
    "\n",
    "        for i in range(size):\n",
    "            predicted_label, predicted_p = self.predict(test_data.iloc[i].to_dict())\n",
    "\n",
    "            # Calculate 0-1 loss function\n",
    "            label = test_label[self.label_name].iloc[i]\n",
    "            if predicted_label != label:\n",
    "                loss_zero_one += 1  # Count incorrect predicted label\n",
    "                # Calculate squared loss function\n",
    "                loss_squared += predicted_p ** 2\n",
    "            else:\n",
    "                # Calculate squared loss function\n",
    "                loss_squared += (1 - predicted_p)**2\n",
    "\n",
    "        correct = size - loss_zero_one\n",
    "        loss_zero_one /= size\n",
    "        loss_squared /= size\n",
    "        self.accuracy = correct / size\n",
    "\n",
    "        print(f\"ZERO-ONE LOSS={loss_zero_one:.4f}\")\n",
    "        print(f\"SQUARED LOSS={loss_squared:.4f} Test Accuracy={self.accuracy:.4f}\")\n",
    "        return loss_zero_one, loss_squared, self.accuracy\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        :param data: single record in dict format\n",
    "        :return: predict_label: predicted label\n",
    "        \"\"\"\n",
    "        predicted_prob = self.priors.copy()\n",
    "        for label in predicted_prob:\n",
    "            for feature in data:\n",
    "                category = self.categories.get(feature, None)\n",
    "                value = data[feature]\n",
    "                if category is None:\n",
    "                    # No need to classify into category data\n",
    "                    predicted_prob[label] = predicted_prob[label] * self.conditional[feature][label][value]\n",
    "                else:\n",
    "                    # Need to calculate the category of the data feature value\n",
    "                    tmp = category.contains(value)\n",
    "                    intervalList = category[tmp]\n",
    "                    value_cat_map = None\n",
    "                    if intervalList.size == 1:\n",
    "                        # Can find a category for the given value\n",
    "                        value_cat_map = intervalList[0]\n",
    "                    else:\n",
    "                        # TODO For real category data, need to handle order issue\n",
    "                        # TODO But for interval, order is fine\n",
    "                        # Check min and max\n",
    "                        minInterval = category[0]\n",
    "                        if (value < minInterval.left):\n",
    "                            # If less than min\n",
    "                            value_cat_map = minInterval\n",
    "                        else:\n",
    "                            # Else assume to be larger than max\n",
    "                            value_cat_map = category[category.size - 1]\n",
    "                    predicted_prob[label] = predicted_prob[label] * self.conditional[feature][label][value_cat_map]\n",
    "\n",
    "        # Calculate actual probability\n",
    "        sum = 0\n",
    "        for label in predicted_prob:\n",
    "            sum += predicted_prob[label]\n",
    "        for label in predicted_prob:\n",
    "            predicted_prob[label] /= sum\n",
    "\n",
    "        # Argmax\n",
    "        max_p = -1\n",
    "        predicted_label = 0\n",
    "        for label in predicted_prob:\n",
    "            if predicted_prob[label] > max_p:\n",
    "                max_p = predicted_prob[label]\n",
    "                predicted_label = label\n",
    "        return predicted_label, max_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}